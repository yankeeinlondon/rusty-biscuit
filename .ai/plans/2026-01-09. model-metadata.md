---
status: "ready_for_approval"
started_at: "2026-01-09"
finished_at:
duration:
tts_gender: male
requirements: |
  Add rich model metadata (modalities, context window, description) to ai-pipeline by:
  1. Fetching from Parsera LLM Specs API at build time
  2. Generating a static metadata lookup table alongside provider enums
  3. Providing runtime accessor methods on ProviderModel

  User clarified:
  - Data source: Parsera API (https://api.parsera.org/v1/llm-specs)
  - Fetch timing: Build time via gen-models binary
  - Priority fields: modalities, context_window, description, default_temperature
required_skills: [rust]
important_skills: {}
subagents: {}
monorepo_modules_impacted: ["ai-pipeline"]
---
# Plan: Model Metadata via Parsera API

## Summary

Extend the `gen-models` binary to fetch metadata from Parsera API and generate a static lookup table. Provides runtime accessors for modalities, context window, and capabilities.

## Parsera API Response

```json
{
  "name": "GPT-4o mini",
  "id": "gpt-4o-mini",
  "provider": "openai",
  "family": "gpt-4o-mini",
  "context_window": 128000,
  "max_output_tokens": 16384,
  "modalities": { "input": ["text", "image"], "output": ["text"] },
  "capabilities": ["function_calling", "structured_output"]
}
```

Note: Parsera does NOT provide `default_temperature` - only Mistral's native API has that.

---

## Phase 1: Runtime Metadata Types

**File**: `ai-pipeline/lib/src/models/model_metadata.rs` (new)

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Modality { Text, Image, Audio, Video, Embeddings }

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ModelModalities {
    pub input: Vec<Modality>,
    pub output: Vec<Modality>,
}

#[derive(Debug, Clone)]
pub struct ModelMetadata {
    pub display_name: String,
    pub family: Option<String>,
    pub context_window: Option<u32>,
    pub max_output_tokens: Option<u32>,
    pub modalities: Option<ModelModalities>,
    pub capabilities: Vec<String>,
}
```

**File**: `ai-pipeline/lib/src/models/mod.rs` - add `pub mod model_metadata;`

### Acceptance Criteria

- [ ] `ModelMetadata` struct defined with all fields
- [ ] `Modality` enum with FromStr impl
- [ ] `cargo check -p ai-pipeline` passes

---

## Phase 2: Parsera API Client

**File**: `ai-pipeline/gen/src/parsera.rs` (new)

```rust
const PARSERA_API_URL: &str = "https://api.parsera.org/v1/llm-specs";

#[derive(Debug, Deserialize)]
pub struct ParseraModel {
    pub id: String,
    pub name: String,
    pub family: Option<String>,
    pub provider: String,
    pub context_window: Option<u32>,
    pub max_output_tokens: Option<u32>,
    pub modalities: Option<ParseraModalities>,
    pub capabilities: Option<Vec<String>>,
}

#[derive(Debug, Deserialize)]
pub struct ParseraModalities {
    pub input: Vec<String>,
    pub output: Vec<String>,
}

pub async fn fetch_parsera_specs() -> Result<Vec<ParseraModel>, ParseraError>;
pub fn index_by_id(models: Vec<ParseraModel>) -> HashMap<String, ParseraModel>;
```

### Error Handling

- API failure: Log warning, continue without metadata
- Timeout: 30 seconds with one retry
- Malformed entries: Skip and log at DEBUG level

### Acceptance Criteria

- [ ] Fetches from Parsera API successfully
- [ ] Returns indexed HashMap for fast lookup
- [ ] Graceful degradation on failure

---

## Phase 3: Metadata Generator

**File**: `ai-pipeline/gen/src/metadata_generator.rs` (new)

Generates `metadata_generated.rs` with static lookup:

```rust
pub static MODEL_METADATA: Lazy<HashMap<&'static str, ModelMetadata>> = Lazy::new(|| {
    let mut m = HashMap::with_capacity(N);
    m.insert("gpt-4o", ModelMetadata {
        display_name: "GPT-4o".to_string(),
        family: Some("gpt-4o".to_string()),
        context_window: Some(128000),
        max_output_tokens: Some(16384),
        modalities: Some(ModelModalities {
            input: vec![Modality::Text, Modality::Image],
            output: vec![Modality::Text],
        }),
        capabilities: vec!["function_calling".to_string()],
    });
    // ... all models
    m
});
```

### Model ID Matching Strategy

1. Exact match on `id`
2. Strip date suffix (`claude-3-5-haiku-20241022` -> `claude-3-5-haiku`)
3. Match via `family` field as fallback

### Acceptance Criteria

- [ ] Generates valid Rust code
- [ ] All provider models included (with or without metadata)
- [ ] Unknown models get empty/None fields

---

## Phase 4: Integrate into gen-models

**File**: `ai-pipeline/gen/src/main.rs`

```rust
mod parsera;
mod metadata_generator;

async fn main() {
    // Fetch Parsera data once at startup
    let parsera_index = match fetch_parsera_specs().await {
        Ok(specs) => {
            info!("Fetched {} model specs from Parsera", specs.len());
            index_by_id(specs)
        }
        Err(e) => {
            warn!("Parsera unavailable: {e}");
            HashMap::new()
        }
    };

    let mut metadata_gen = MetadataGenerator::new();

    // During provider loop, collect metadata
    for provider in providers {
        for model_id in &models {
            let parsera_data = find_parsera_metadata(model_id, &parsera_index);
            metadata_gen.register(model_id.clone(), parsera_data.cloned());
        }
    }

    // After all providers, generate metadata file
    let metadata_code = metadata_gen.generate();
    write_atomic(&output_dir.join("metadata_generated.rs"), &metadata_code)?;
}
```

### Acceptance Criteria

- [ ] Parsera fetch happens once before provider loop
- [ ] Metadata file generated alongside provider enums
- [ ] Works even if Parsera API is unavailable

---

## Phase 5: Runtime Accessor API

**File**: `ai-pipeline/lib/src/rigging/providers/models/mod.rs`

```rust
mod metadata_generated;

impl ProviderModel {
    /// Returns metadata for this model if available.
    pub fn metadata(&self) -> Option<&'static ModelMetadata> {
        metadata_generated::MODEL_METADATA.get(self.model_id())
    }

    /// Returns the context window size if known.
    pub fn context_window(&self) -> Option<u32> {
        self.metadata().and_then(|m| m.context_window)
    }

    /// Returns true if this model supports the given input modality.
    pub fn supports_input(&self, modality: Modality) -> bool {
        self.metadata()
            .and_then(|m| m.modalities.as_ref())
            .map(|mods| mods.input.contains(&modality))
            .unwrap_or(false)
    }

    /// Returns true if this model supports the given output modality.
    pub fn supports_output(&self, modality: Modality) -> bool {
        self.metadata()
            .and_then(|m| m.modalities.as_ref())
            .map(|mods| mods.output.contains(&modality))
            .unwrap_or(false)
    }
}
```

### Acceptance Criteria

- [ ] `ProviderModel::metadata()` returns correct data
- [ ] Convenience methods work for known models
- [ ] Returns None/false for unknown models

---

## Phase 6: Dependencies

**File**: `ai-pipeline/lib/Cargo.toml`

```toml
once_cell = "1.19"  # For Lazy static
```

**File**: `ai-pipeline/gen/Cargo.toml` - already has reqwest, serde

---

## Files Summary

| File | Action |
|------|--------|
| `lib/src/models/model_metadata.rs` | Create - runtime types |
| `lib/src/models/mod.rs` | Modify - add module |
| `gen/src/parsera.rs` | Create - API client |
| `gen/src/metadata_generator.rs` | Create - code gen |
| `gen/src/main.rs` | Modify - integrate Parsera |
| `lib/src/rigging/providers/models/mod.rs` | Modify - accessor API |
| `lib/src/rigging/providers/models/metadata_generated.rs` | Generated |
| `lib/Cargo.toml` | Modify - add once_cell |

---

## Verification

1. `cargo build -p ai-pipeline-gen` - generator compiles
2. `gen-models --dry-run -v` - see metadata output
3. `gen-models` - generate all files
4. `cargo test -p ai-pipeline-lib` - accessor tests pass
5. Spot-check: `gpt-4o` has context_window = 128000
