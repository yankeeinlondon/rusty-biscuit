---
status: "ready_for_approval"
started_at: "2026-01-09"
finished_at:
duration:
tts_gender: male
requirements: |
  Build a system to generate Rust enums for provider models by:
  1. Calling OpenAI-compatible `/v1/models` API for each provider
  2. Using `ProviderConfig` to get URLs and authentication methods
  3. Generating enum variants from wire-format model IDs
  4. Output to `ai-pipeline/lib/src/rigging/providers/models/{provider}.rs`

  User clarified:
  - Architecture: Standalone binary crate
  - Error handling: Skip and continue on failures
  - Refactor openai_api.rs to use Provider.config() methods
  - Add missing provider configs (Groq, HuggingFace, Mira, Mistral)
required_skills: [clap, rust]
important_skills: {}
subagents: {}
monorepo_modules_impacted: ["ai-pipeline"]
---
# Plan: Provider Model Enum Generator

## Summary

Build a standalone binary `gen-models` that fetches model lists from LLM provider APIs and generates Rust enum files with `ModelId` derive support.

## Execution Order

```
┌─────────────────┐    ┌─────────────────┐
│  Phase 1        │    │  Phase 3        │
│  Provider Configs│    │  Binary Crate   │
└────────┬────────┘    └────────┬────────┘
         │                      │
         │  (parallel)          │
         └──────────┬───────────┘
                    ▼
         ┌─────────────────────┐
         │      Phase 2        │
         │  Refactor API       │
         └──────────┬──────────┘
                    ▼
         ┌─────────────────────┐
         │      Phase 4        │
         │  Code Generation    │
         └──────────┬──────────┘
                    ▼
    ┌───────────────┴───────────────┐
    │           (parallel)          │
    ▼                               ▼
┌─────────────┐            ┌─────────────┐
│  Phase 5    │            │  Phase 6    │
│  Testing    │            │  Docs       │
└─────────────┘            └─────────────┘
```

---

## Phase 1: Add Missing Provider Configs

**File**: `ai-pipeline/lib/src/rigging/providers/providers.rs`

**Objective**: Add `ProviderConfig` entries for Groq, Mistral, HuggingFace, and Mira.

### Configs to Add

```rust
m.insert(Provider::Groq, ProviderConfig {
    env_vars: &["GROQ_API_KEY"],
    auth_method: ApiAuthMethod::BearerToken,
    base_url: "https://api.groq.com/openai",  // Note: /openai prefix required
    models_endpoint: None,
    is_local: false,
});

m.insert(Provider::Mistral, ProviderConfig {
    env_vars: &["MISTRAL_API_KEY"],
    auth_method: ApiAuthMethod::BearerToken,
    base_url: "https://api.mistral.ai",
    models_endpoint: None,
    is_local: false,
});

m.insert(Provider::HuggingFace, ProviderConfig {
    env_vars: &["HF_TOKEN", "HUGGINGFACE_TOKEN", "HUGGING_FACE_TOKEN"],
    auth_method: ApiAuthMethod::BearerToken,
    base_url: "https://huggingface.co/api",
    models_endpoint: Some("/models"),  // Non-standard: returns paginated list
    is_local: false,
});

m.insert(Provider::Mira, ProviderConfig {
    env_vars: &["MIRA_API_KEY"],
    auth_method: ApiAuthMethod::BearerToken,
    base_url: "https://api.mira.network",  // NEEDS VERIFICATION
    models_endpoint: None,
    is_local: false,
});
```

### Notes

- **Groq**: Base URL must include `/openai` for OpenAI-compatible endpoint
- **HuggingFace**: Uses non-standard `/api/models` endpoint with pagination - may need custom handler
- **Mira**: API not well-documented; mark as experimental/skip if unavailable

### Acceptance Criteria

- [ ] `Provider::Groq.config()` returns valid config
- [ ] `Provider::Mistral.config()` returns valid config
- [ ] `Provider::HuggingFace.config()` returns valid config
- [ ] `Provider::Mira.config()` returns valid config (or marked experimental)
- [ ] `cargo check -p ai-pipeline` passes

---

## Phase 2: Refactor openai_api.rs

**File**: `ai-pipeline/lib/src/api/openai_api.rs`

**Objective**: Replace legacy lookup maps with `Provider.config()` methods.

### Current Issues

The file references undefined symbols:

- `PROVIDER_BASE_URLS` - replace with `provider.base_url()`
- `PROVIDER_MODELS_ENDPOINT` - replace with `provider.models_endpoint()`
- `build_auth_header` - implement using `provider.config().auth_method`
- Missing imports: `Provider`, `ProviderError`, `OpenAIModelsResponse`, etc.

### Implementation

1. **Add imports**:

```rust
use crate::rigging::providers::providers::Provider;
use crate::api::auth::ApiAuthMethod;
```

1. **Replace lookups**:

```rust
// Before
let Some(base_url) = PROVIDER_BASE_URLS.get(&provider) else { ... };

// After
let base_url = provider.base_url();
let endpoint = provider.models_endpoint();
```

1. **Implement build_auth_header**:

```rust
fn build_auth_header(provider: &Provider, api_key: &str) -> (String, String) {
    match provider.config().auth_method {
        ApiAuthMethod::BearerToken => ("Authorization".to_string(), format!("Bearer {}", api_key)),
        ApiAuthMethod::ApiKey(ref header) => (header.clone(), api_key.to_string()),
        ApiAuthMethod::QueryParam(_) => (String::new(), String::new()), // Handled in URL
        ApiAuthMethod::None => (String::new(), String::new()),
    }
}
```

1. **Define response types** (if not already defined elsewhere):

```rust
#[derive(Debug, serde::Deserialize)]
struct OpenAIModelsResponse {
    data: Vec<OpenAIModel>,
}

#[derive(Debug, serde::Deserialize)]
struct OpenAIModel {
    id: String,
}
```

### Acceptance Criteria

- [ ] No references to legacy lookup maps
- [ ] All functions use `Provider.config()` exclusively
- [ ] `cargo check -p ai-pipeline` passes
- [ ] Existing tests pass

---

## Phase 3: Create Binary Crate (Parallel with Phase 1)

**Location**: `ai-pipeline/gen/`

### Structure

```
ai-pipeline/gen/
├── Cargo.toml
└── src/
    ├── main.rs
    ├── generator.rs
    └── errors.rs
```

### Cargo.toml

```toml
[package]
name = "ai-pipeline-gen"
version = "0.1.0"
edition = "2021"  # Note: NOT 2024

[[bin]]
name = "gen-models"
path = "src/main.rs"

[dependencies]
ai-pipeline = { path = "../lib" }
clap = { version = "4.5", features = ["derive"] }
tokio = { version = "1.49", features = ["macros", "rt-multi-thread"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
prettyplease = "0.2"
syn = { version = "2.0", features = ["full", "parsing"] }
chrono = "0.4"
thiserror = "2.0"
```

### CLI Interface (main.rs)

```rust
use clap::Parser;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "gen-models")]
#[command(about = "Generate provider model enum files from API")]
struct Cli {
    /// Output directory for generated files
    #[arg(short, long, default_value = "ai-pipeline/lib/src/rigging/providers/models")]
    output: PathBuf,

    /// Specific providers to generate (comma-separated)
    #[arg(short, long)]
    providers: Option<String>,

    /// Skip specific providers (comma-separated)
    #[arg(short, long)]
    skip: Option<String>,

    /// Verbosity level (-v, -vv, -vvv)
    #[arg(short, long, action = clap::ArgAction::Count)]
    verbose: u8,
}
```

### Update Workspace

Add to `/Volumes/coding/personal/dockhand/Cargo.toml`:

```toml
members = [
    "ai-pipeline/lib",
    "ai-pipeline/gen",  # Add this
    # ... other members
]
```

### Acceptance Criteria

- [ ] `cargo build -p ai-pipeline-gen` succeeds
- [ ] `cargo run -p ai-pipeline-gen -- --help` shows usage
- [ ] Binary is named `gen-models`

---

## Phase 4: Implement Code Generation

**Files**: `ai-pipeline/gen/src/generator.rs`, `ai-pipeline/gen/src/errors.rs`

### Error Types (errors.rs)

```rust
use thiserror::Error;

#[derive(Debug, Error)]
pub enum GeneratorError {
    #[error("Failed to fetch models from {provider}: {reason}")]
    FetchFailed { provider: String, reason: String },

    #[error("Failed to write output file {path}: {reason}")]
    WriteFailed { path: String, reason: String },

    #[error("Code generation failed for {provider}: {reason}")]
    CodegenFailed { provider: String, reason: String },
}
```

### Generator (generator.rs)

```rust
use ai_pipeline::rigging::providers::models::build::enum_name::enum_variant_name_from_wire_id;
use std::collections::HashSet;

pub struct ModelEnumGenerator {
    provider_name: String,
    models: Vec<String>,
}

impl ModelEnumGenerator {
    pub fn new(provider_name: String, models: Vec<String>) -> Self {
        // Deduplicate and sort
        let unique: HashSet<_> = models.into_iter().collect();
        let mut models: Vec<_> = unique.into_iter().collect();
        models.sort();
        Self { provider_name, models }
    }

    pub fn generate(&self) -> String {
        let enum_name = format!("ProviderModel{}", self.provider_name);
        let variants = self.generate_variants();

        format!(r#"//! Auto-generated provider model enum
//!
//! Generated: {timestamp}
//! Generator: gen-models v{version}
//! Provider: {provider}
//!
//! Do not edit manually.

use model_id::ModelId;

#[allow(non_camel_case_types)]
#[derive(Debug, Clone, PartialEq, Eq, Hash, ModelId)]
pub enum {enum_name} {{
{variants}
    Bespoke(String),
}}
"#,
            timestamp = chrono::Utc::now().to_rfc3339(),
            version = env!("CARGO_PKG_VERSION"),
            provider = self.provider_name,
            enum_name = enum_name,
            variants = variants,
        )
    }

    fn generate_variants(&self) -> String {
        self.models
            .iter()
            .map(|model_id| {
                let variant = enum_variant_name_from_wire_id(model_id);
                format!("    {},", variant)
            })
            .collect::<Vec<_>>()
            .join("\n")
    }
}
```

### Main Processing Logic

```rust
async fn process_providers(providers: Vec<Provider>, output_dir: &Path) -> GenerationSummary {
    let api_keys = get_api_keys();
    let mut summary = GenerationSummary::default();

    for provider in providers {
        match process_single_provider(provider, &api_keys, output_dir).await {
            Ok(count) => {
                info!("Generated {} models for {:?}", count, provider);
                summary.succeeded.push((provider, count));
            }
            Err(e) => {
                warn!("Skipping {:?}: {}", provider, e);
                summary.skipped.push((provider, e.to_string()));
            }
        }
    }

    summary
}
```

### Key Implementation Details

1. **Use `enum_variant_name_from_wire_id`**: Takes `&str`, doesn't need `ModelDefinition`
2. **Deduplicate with HashSet**: Some providers return duplicate model IDs
3. **Sort alphabetically**: Consistent output across runs
4. **Atomic writes**: Write to `.tmp` file, then rename
5. **Skip on failure**: Log warning, continue with next provider

### Atomic Write Pattern

```rust
fn write_atomic(path: &Path, content: &str) -> std::io::Result<()> {
    let tmp_path = path.with_extension("rs.tmp");
    std::fs::write(&tmp_path, content)?;
    std::fs::rename(&tmp_path, path)?;
    Ok(())
}
```

### Update models/mod.rs

After generating provider files, update `ai-pipeline/lib/src/rigging/providers/models/mod.rs`:

```rust
// Generated modules
pub mod anthropic;
pub mod deepseek;
pub mod gemini;
pub mod groq;      // New
pub mod mistral;   // New
// etc.
```

### Acceptance Criteria

- [ ] Generates one `.rs` file per provider
- [ ] Files match existing format (use `model_id::ModelId`)
- [ ] Includes `Bespoke(String)` variant
- [ ] Failed providers logged as warnings, not fatal errors
- [ ] Output properly formatted
- [ ] Atomic writes prevent partial files
- [ ] `models/mod.rs` updated with new modules

---

## Phase 5: Testing (Parallel with Phase 6)

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_variant_generation() {
        let generator = ModelEnumGenerator::new(
            "OpenAi".to_string(),
            vec!["gpt-4o".to_string(), "gpt-4o-mini".to_string()],
        );
        let code = generator.generate();

        assert!(code.contains("Gpt__4o,"));
        assert!(code.contains("Gpt__4o__Mini,"));
        assert!(code.contains("Bespoke(String)"));
    }

    #[test]
    fn test_deduplication() {
        let generator = ModelEnumGenerator::new(
            "Test".to_string(),
            vec!["model-a".to_string(), "model-a".to_string()],
        );
        let code = generator.generate();

        assert_eq!(code.matches("Model__A,").count(), 1);
    }

    #[test]
    fn test_generated_code_parses() {
        let generator = ModelEnumGenerator::new(
            "Test".to_string(),
            vec!["test-model".to_string()],
        );
        let code = generator.generate();

        syn::parse_file(&code).expect("Generated code should parse");
    }
}
```

### Integration Test

```rust
#[tokio::test]
#[ignore]  // Requires network and API keys
async fn test_full_generation_workflow() {
    use tempfile::TempDir;

    let temp_dir = TempDir::new().unwrap();
    let summary = process_providers(vec![Provider::OpenAi], temp_dir.path()).await;

    // Verify file was created (if API key available)
    if !summary.succeeded.is_empty() {
        assert!(temp_dir.path().join("openai.rs").exists());
    }
}
```

### Acceptance Criteria

- [ ] Unit tests pass
- [ ] Integration tests pass with `--ignored` flag
- [ ] Generated files compile with `cargo check`

---

## Phase 6: Documentation (Parallel with Phase 5)

### README (ai-pipeline/gen/README.md)

```markdown
# gen-models

Generator for provider model enum files.

## Usage

\`\`\`bash
# Generate all providers
cargo run -p ai-pipeline-gen

# Generate specific providers
cargo run -p ai-pipeline-gen -- --providers openai,anthropic

# Skip specific providers
cargo run -p ai-pipeline-gen -- --skip zenmux,ollama

# Custom output directory
cargo run -p ai-pipeline-gen -- --output ./models
\`\`\`

## Environment Variables

Set API keys for providers you want to generate:

| Provider | Variable |
|----------|----------|
| OpenAI | `OPENAI_API_KEY` |
| Anthropic | `ANTHROPIC_API_KEY` |
| Groq | `GROQ_API_KEY` |
| Mistral | `MISTRAL_API_KEY` |
| etc. | ... |
```

### Justfile Target

Add to `ai-pipeline/justfile`:

```justfile
# Generate provider model enums from APIs
gen-models:
    cargo run -p ai-pipeline-gen

# Generate for specific provider
gen-models-for provider:
    cargo run -p ai-pipeline-gen -- --providers {{provider}}
```

### Acceptance Criteria

- [ ] README documents usage and env vars
- [ ] Justfile targets work
- [ ] `--help` output is useful

---

## Critical Files Summary

| File | Action |
|------|--------|
| `ai-pipeline/lib/src/rigging/providers/providers.rs` | Add 4 provider configs |
| `ai-pipeline/lib/src/api/openai_api.rs` | Refactor to use Provider.config() |
| `ai-pipeline/lib/src/rigging/providers/models/build/enum_name.rs` | Reference only |
| `ai-pipeline/lib/src/rigging/providers/models/mod.rs` | Update with new modules |
| `ai-pipeline/gen/Cargo.toml` | Create new |
| `ai-pipeline/gen/src/main.rs` | Create new |
| `ai-pipeline/gen/src/generator.rs` | Create new |
| `ai-pipeline/gen/src/errors.rs` | Create new |
| `ai-pipeline/gen/README.md` | Create new |
| `ai-pipeline/justfile` | Add targets |
| `Cargo.toml` (workspace) | Add member |

---

## Verification

After implementation, verify:

1. **Build**: `cargo build -p ai-pipeline-gen`
2. **Run**: `cargo run -p ai-pipeline-gen -- --help`
3. **Generate**: With API keys set, run `just gen-models` from ai-pipeline/
4. **Check output**: Inspect generated `.rs` files
5. **Compile generated code**: `cargo check -p ai-pipeline`
6. **Run tests**: `cargo test -p ai-pipeline-gen`

---

## Lessons Learned

- [FILE: ai-pipeline/lib/Cargo.toml]: Uses edition "2024" which doesn't exist - should be "2021"
- [FILE: ai-pipeline/lib/src/api/openai_api.rs]: References undefined symbols (PROVIDER_BASE_URLS, etc.) - file may be incomplete/placeholder

## Package Changes in Execution

- [ADD]: chrono in cargo - timestamp generation for file headers
- [ADD]: prettyplease in cargo - Rust code formatting (optional, for prettier output)
- [ADD]: thiserror in cargo - error type derivation (already in ai-pipeline/lib)
